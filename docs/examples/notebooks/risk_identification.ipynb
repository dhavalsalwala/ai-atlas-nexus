{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook explains how to uncover risks related to your usecase based on a given taxonomy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhaval/Projects/Usage-Governance/ai-atlas-nexus/src/ai_atlas_nexus/blocks/risk_explorer/explorer.py:8: UserWarning: Deprecated, Please use AtlasExplorer instead\n",
      "  warnings.warn(\"Deprecated, Please use AtlasExplorer instead\")\n",
      "/Users/dhaval/Projects/Usage-Governance/ai-atlas-nexus/src/ai_atlas_nexus/toolkit/job_utils.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from ai_atlas_nexus.blocks.inference import (\n",
    "    RITSInferenceEngine,\n",
    "    WMLInferenceEngine,\n",
    "    OllamaInferenceEngine,\n",
    "    VLLMInferenceEngine,\n",
    ")\n",
    "from ai_atlas_nexus.blocks.inference.params import (\n",
    "    InferenceEngineCredentials,\n",
    "    RITSInferenceEngineParams,\n",
    "    WMLInferenceEngineParams,\n",
    "    OllamaInferenceEngineParams,\n",
    "    VLLMInferenceEngineParams,\n",
    ")\n",
    "from ai_atlas_nexus.library import AIAtlasNexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AI Atlas Nexus uses Large Language Models (LLMs) to infer risks dimensions. Therefore requires access to LLMs to inference or call the model.\n",
    "\n",
    "**Available Inference Engines**: WML, Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/ai-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
    "\n",
    "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-16 12:24:54:241] - INFO - AIAtlasNexus - Created RITS inference engine.\n"
     ]
    }
   ],
   "source": [
    "# inference_engine = OllamaInferenceEngine(\n",
    "#     model_name_or_path=\"granite3.3:8b\",\n",
    "#     credentials=InferenceEngineCredentials(api_url=\"OLLAMA_API_URL\"),\n",
    "#     parameters=OllamaInferenceEngineParams(\n",
    "#         num_predict=1000, num_ctx=8192, temperature=0, repeat_penalty=1\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# inference_engine = WMLInferenceEngine(\n",
    "#     model_name_or_path=\"ibm/granite-3-3-8b-instruct\",\n",
    "#     credentials={\n",
    "#         \"api_key\": \"WML_API_KEY\",\n",
    "#         \"api_url\": \"WML_API_URL\",\n",
    "#         \"project_id\": \"WML_PROJECT_ID\",\n",
    "#     },\n",
    "#     parameters=WMLInferenceEngineParams(\n",
    "#         max_new_tokens=1000, decoding_method=\"greedy\", repetition_penalty=1\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# inference_engine = VLLMInferenceEngine(\n",
    "#     model_name_or_path=\"ibm-granite/granite-3.3-8b-instruct\",\n",
    "#     credentials=InferenceEngineCredentials(\n",
    "#         api_url=\"VLLM_API_URL\", api_key=\"VLLM_API_KEY\"\n",
    "#     ),\n",
    "#     parameters=VLLMInferenceEngineParams(max_tokens=1000, temperature=0),\n",
    "# )\n",
    "\n",
    "inference_engine = RITSInferenceEngine(\n",
    "    model_name_or_path=\"ibm-granite/granite-3.3-8b-instruct\",\n",
    "    credentials={\n",
    "        \"api_key\": \"cbc683b3a1a7c52d2a73008b785d2811\",\n",
    "        \"api_url\": \"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com\",\n",
    "    },\n",
    "    parameters=RITSInferenceEngineParams(\n",
    "        max_completion_tokens=1000, temperature=0, logprobs=True, top_logprobs=3\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an instance of AIAtlasNexus\n",
    "\n",
    "_Note: (Optional)_ You can specify your own directory in `AIAtlasNexus(base_dir=<PATH>)` to utilize custom AI ontologies. If left blank, the system will use the provided AI ontologies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-16 12:24:54:504] - INFO - AIAtlasNexus - Created AIAtlasNexus instance. Base_dir: None\n"
     ]
    }
   ],
   "source": [
    "ai_atlas_nexus = AIAtlasNexus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Risk Identification API\n",
    "\n",
    "AIAtlasNexus.identify_risks_from_usecases()\n",
    "\n",
    "Params:\n",
    "\n",
    "- usecases (List[str]):\n",
    "  A List of strings describing AI usecases\n",
    "- inference_engine (InferenceEngine):\n",
    "  An LLM inference engine to infer risks from the usecases.\n",
    "- taxonomy (str, optional):\n",
    "  The string label for a taxonomy. Default to None.\n",
    "- cot_examples (Dict[str, List], optional):\n",
    "  The Chain of Thought (CoT) examples to use in the risk identification.\n",
    "  The example template is available at src/ai_atlas_nexus/data/templates/risk_generation_cot.json.\n",
    "  Assign the ID of the taxonomy you wish to use as the key for CoT examples. Providing this value\n",
    "  will override the CoT examples present in the template master. Default to None.\n",
    "- max_risk (int, optional):\n",
    "  The maximum number of risks to extract. Pass None to allow the inference engine to determine the number of risks. Defaults to None.\n",
    "- zero_shot_only (bool): If enabled, this flag allows the system to perform Zero Shot Risk identification, and the field `cot_examples` will be ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Risk Identification using IBM AI Risk taxonomy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4b2f7a3a7b40b0b3125f1c2a578b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "Over- or under-reliance\n",
      "Confidential data in prompt\n",
      "Data privacy rights alignment\n",
      "Discriminatory actions\n",
      "Legal accountability\n",
      "Hallucination\n",
      "Mitigation and maintenance\n",
      "AI agent compliance\n",
      "Function calling hallucination\n",
      "Confidential information in data\n",
      "Lack of model transparency\n",
      "Unrepresentative data\n",
      "AI agents' impact on human agency\n",
      "Personal information in prompt\n",
      "Sharing IP/PI/confidential information with user\n",
      "Lack of testing diversity\n",
      "Decision bias\n",
      "Exposing personal information\n",
      "Improper data curation\n",
      "Over- or under-reliance on AI agents\n",
      "Revealing confidential information\n",
      "Uncertain data provenance\n",
      "Data bias\n",
      "Unauthorized use\n",
      "Misaligned actions\n",
      "Incomplete usage definition\n",
      "Lack of data transparency\n",
      "Impact on affected communities\n",
      "Improper retraining\n",
      "Introduce data bias\n",
      "Accountability of AI agent actions\n",
      "Incomplete AI agent evaluation\n",
      "Inaccessible training data\n",
      "Non-disclosure\n",
      "Lack of training data transparency\n",
      "Reproducibility\n",
      "Incomplete advice\n",
      "Prompt injection attack\n",
      "Lack of system transparency\n",
      "Personal information in data\n",
      "Extraction attack\n",
      "Data acquisition restrictions\n",
      "Sharing IP/PI/confidential information with tools\n",
      "Prompt priming\n",
      "Reidentification\n",
      "Attribute inference attack\n",
      "Poor model accuracy\n",
      "Generated content ownership and IP\n",
      "Lack of AI agent transparency\n",
      "Impact on human dignity\n",
      "Output bias\n",
      "Unexplainable output\n",
      "Unexplainable and untraceable actions\n",
      "Unreliable source attribution\n",
      "Lack of domain expertise\n",
      "Exclusion\n"
     ]
    }
   ],
   "source": [
    "usecase = \"Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers.\"\n",
    "\n",
    "risks = ai_atlas_nexus.identify_risks_from_usecases(\n",
    "    usecases=[usecase],\n",
    "    inference_engine=inference_engine,\n",
    "    taxonomy=\"ibm-risk-atlas\",\n",
    "    max_risk=5,\n",
    ")\n",
    "\n",
    "print(len(risks[0]))\n",
    "for risk in risks[0]:\n",
    "    print(risk.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Risk Identification using IBM AI Risk taxonomy with Custom CoT examples\n",
    "\n",
    "**Note:** To use custom risk `cot_examples` for a new taxonomy or an existing taxonomy, users must provide their own set of example usecase and associated risks in a JSON file such as in [risk_generation_cot.json](https://github.com/IBM/ai-atlas-nexus/blob/main/src/ai_atlas_nexus/data/templates/risk_generation_cot.json). This will enable the LLM to learn from these few-shot examples and generate better responses. Please follow the guide [here](https://ibm.github.io/ai-atlas-nexus/concepts/Chain_of_thought_templates/#risk-identification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a81c3b540444eed83414671622143eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[Risk(id='atlas-over-or-under-reliance', name='Over- or under-reliance', description='In AI-assisted decision-making tasks, reliance measures how much a person trusts (and potentially acts on) a model’s output. Over-reliance occurs when a person puts too much trust in a model, accepting a model’s output when the model’s output is likely incorrect. Under-reliance is the opposite, where the person doesn’t trust the model but should.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/over-or-under-reliance.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-value-alignment', closeMatch=['credo-risk-016'], exactMatch=[], broadMatch=['nist-human-ai-configuration'], narrowMatch=[], relatedMatch=['llm052025-improper-output-handling', 'llm062025-excessive-agency', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-5.1'], detectsRiskConcept=[], tag='over-or-under-reliance', type='output', phase=None, descriptor=['amplified by generative AI'], concern='In tasks where humans make choices based on AI-based suggestions, over/under reliance can lead to poor decision making because of the misplaced trust in the AI system, with negative consequences that increase with the importance of the decision.'),\n",
       "  Risk(id='atlas-confidential-data-in-prompt', name='Confidential data in prompt', description='Confidential information might be included as a part of the prompt that is sent to the model.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/confidential-data-in-prompt.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-intellectual-property', closeMatch=[], exactMatch=[], broadMatch=['nist-intellectual-property'], narrowMatch=[], relatedMatch=['ail-privacy', 'llm022025-sensitive-information-disclosure', 'mit-ai-causal-risk-entity-other', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.1'], detectsRiskConcept=[], tag='confidential-data-in-prompt', type='inference', phase=None, descriptor=['specific to generative AI'], concern=\"If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output. Additionally, end users' confidential information might be unintentionally collected and stored.\"),\n",
       "  Risk(id='atlas-data-privacy-rights', name='Data privacy rights alignment', description='Applicable laws can establish data subject rights such as opt-out rights, right to access, and right to be forgotten.\\xa0Synthetic data might raise unique concerns, such as the potential for reidentification of individuals from seemingly anonymous synthetic data. Data subject rights might also be relevant in scenarios where synthetic data is derived from sensitive or personal information.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-privacy-rights.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=['nist-data-privacy'], narrowMatch=[], relatedMatch=['ail-intellectual-property'], detectsRiskConcept=[], tag='data-privacy-rights', type='training-data', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='Improper usage or a request for data removal could force organizations to retrain the model, which might be expensive.'),\n",
       "  Risk(id='atlas-discriminatory-actions-agentic', name='Discriminatory actions', description='AI agents can take actions where one group of humans is unfairly advantaged over another due to the decisions of the model. This may be caused by AI agents’ biased actions that impact the world, in the resources consulted, and in the resource selection process. For example, an AI agent can generate code that can be biased.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/discriminatory-actions-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-fairness', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='discriminatory-actions-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Discriminatory actions can cause harm to people. Discriminatory actions taken by an AI agent could perpetuate bias to systems outside the AI agent owner’s control,  impact people, or lead to unintended consequences.'),\n",
       "  Risk(id='atlas-legal-accountability', name='Legal accountability', description='Determining who is responsible for an AI model is challenging without good documentation and governance processes. The use of synthetic data in model development adds further complexity, since the lack of standardized frameworks for recording synthetic data design choices and verification steps makes accountability harder to establish. ', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/legal-accountability.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-legal-compliance', closeMatch=[], exactMatch=[], broadMatch=['nist-data-privacy', 'nist-intellectual-property', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['credo-risk-023', 'credo-risk-034', 'credo-risk-046', 'mit-ai-causal-risk-entity-other', 'mit-ai-causal-risk-intent-other', 'mit-ai-causal-risk-timing-other', 'mit-ai-risk-subdomain-6.5'], detectsRiskConcept=[], tag='legal-accountability', type='non-technical', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='If ownership for development of the model is uncertain, it may not be clear who would be liable and responsible for the problems with it or can answer questions about it. Users of models without clear ownership might find challenges with compliance with regulations.'),\n",
       "  Risk(id='atlas-hallucination', name='Hallucination', description='Hallucinations generate factually inaccurate or untruthful content relative to the model’s training data or input. Hallucinations are also sometimes referred to lack of faithfulness or lack of groundedness. In some instances, synthetic data that is generated by large language models might include hallucinations that result in the data possibly being inaccurate, fabricated, or disconnected from reality. Hallucinations can compromise model performance, accuracy, and relevance.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/hallucination.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-robustness', closeMatch=[], exactMatch=['nist-confabulation'], broadMatch=[], narrowMatch=[], relatedMatch=['granite-function-call', 'granite-answer-relevance', 'granite-relevance', 'granite-groundedness', 'llm092025-misinformation', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-3.1'], detectsRiskConcept=[], tag='hallucination', type='output', phase=None, descriptor=['specific to generative AI', 'amplified by synthetic data'], concern='Hallucinations can be misleading. These false outputs can mislead users and be incorporated into downstream artifacts, further spreading misinformation. False output can harm both owners and users of the AI models. In some uses, hallucinations can be particularly consequential. Hallucination can introduce fabricated or unrealistic data, a lack of connection to real-world patterns, and decreased predictive power for the foundation model.'),\n",
       "  Risk(id='atlas-mitigation-maintenance-agentic', name='Mitigation and maintenance', description='The large number of components and dependencies that agent systems have complicates keeping them up to date and correcting problems.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/mitigation-maintenance-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='mitigation-maintenance-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='AI agents may interact with other systems, tools, or other agents. Tracing the root cause of failure becomes more difficult and more costly as agent capabilities and complexities increase.'),\n",
       "  Risk(id='atlas-ai-agent-compliance-agentic', name='AI agent compliance', description=\"Determining AI agents' compliance is complex and there might not be enough information to assess whether the agentic AI system is compliant with applicable legal requirements.\", url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/ai-agent-compliance-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='ai-agent-compliance-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='AI agents may interact with other systems, tools, or other agents. AI agents can also find solutions to accomplish a task or a goal in a variety of ways and there could be uncertainty around the way an AI agent would choose each time to perform the task. Assessing compliance can become more difficult as agent capabilities increase.'),\n",
       "  Risk(id='atlas-function-calling-hallucination-agentic', name='Function calling hallucination', description='AI agents might make mistakes when generating function calls (calls to tools to execute actions). Those function calls might result in incorrect, unnecessary or harmful actions. Examples: Generating wrong functions or wrong parameters for the functions.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/function-calling-hallucination-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-robustness', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='function-calling-hallucination-agentic', type='agentic', phase=None, descriptor=['specific to agentic AI'], concern='Hallucinations when generating function calls might result in wrong or redundant actions being performed. Depending on the actions taken, AI agents can cause harms to owners and users of the AI agents.'),\n",
       "  Risk(id='atlas-confidential-information-in-data', name='Confidential information in data', description='Confidential information might be included as part of the data that is used to train or tune the model.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/confidential-information-in-data.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-intellectual-property', closeMatch=[], exactMatch=[], broadMatch=['nist-intellectual-property'], narrowMatch=[], relatedMatch=['ail-intellectual-property', 'llm022025-sensitive-information-disclosure', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-2.1'], detectsRiskConcept=[], tag='confidential-information-in-data', type='training-data', phase=None, descriptor=['amplified by generative AI'], concern='If confidential data is not properly protected, there could be an unwanted disclosure of confidential information. The model might expose confidential information in the generated output or to unauthorized users.'),\n",
       "  Risk(id='atlas-lack-of-model-transparency', name='Lack of model transparency', description='Lack of model transparency is due to insufficient documentation of the model design, development, and evaluation process and the absence of insights into the inner workings of the model.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-model-transparency.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=['nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-other', 'mit-ai-risk-subdomain-7.4'], detectsRiskConcept=[], tag='lack-of-model-transparency', type='non-technical', phase=None, descriptor=['traditional risk of AI'], concern='Transparency is important for legal compliance, AI ethics, and guiding appropriate use of models. Missing information might make it more difficult to evaluate risks,  change the model, or reuse it.\\xa0 Knowledge about who built a model can also be an important factor in deciding whether to trust it. Additionally, transparency regarding how the model’s risks were determined, evaluated, and mitigated also play a role in determining model risks, identifying model suitability, and governing model usage.'),\n",
       "  Risk(id='atlas-unrepresentative-data', name='Unrepresentative data', description='Unrepresentative data occurs when the training or fine-tuning data is not sufficiently representative of the underlying population or does not measure the phenomenon of interest. Synthetic data might not fully capture the complexity and nuances of real-world data. Causes include possible limitations in the seed data quality, biases in generation methods, or inadequate domain knowledge. Thus, AI models might struggle to generalize effectively to real-world scenarios.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unrepresentative-data.html', dateCreated=datetime.date(2024, 9, 24), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-accuracy', closeMatch=[], exactMatch=[], broadMatch=['nist-harmful-bias-or-homogenization'], narrowMatch=[], relatedMatch=['credo-risk-010', 'mit-ai-causal-risk-entity-other', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-7.3', 'nist-value-chain-and-component-integration'], detectsRiskConcept=[], tag='unrepresentative-data', type='training-data', phase=None, descriptor=['traditional risk of AI', 'amplified by synthetic data'], concern='If the data is not representative, then the model will not work as intended.'),\n",
       "  Risk(id='atlas-impact-human-agency-agentic', name=\"AI agents' impact on human agency\", description='The autonomous nature of AI agents in performing tasks or taking actions could affect the individuals’ ability to engage in critical thinking, make choices and act independently.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/impact-human-agency-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-societal-impact', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='impact-human-agency-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='AI agents might shift the decision, thinking, and control from humans to machines.  This might negatively impact the society and human welfare as they limit the freedom and meaningful participations of humans in performing a task or making decisions. '),\n",
       "  Risk(id='atlas-personal-information-in-prompt', name='Personal information in prompt', description='Personal information or sensitive personal information that is included as a part of a prompt that is sent to the model.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/personal-information-in-prompt.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=['nist-data-privacy'], narrowMatch=[], relatedMatch=['llm022025-sensitive-information-disclosure', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.1'], detectsRiskConcept=[], tag='personal-information-in-prompt', type='inference', phase=None, descriptor=['specific to generative AI'], concern='If personal information or sensitive personal information is included in the prompt, it might be unintentionally disclosed in the models’ output. In addition to accidental disclosure, prompt data might be stored or later used for other purposes like model evaluation and retraining, and might appear in their output if not properly removed.\\xa0'),\n",
       "  Risk(id='atlas-sharing-info-user-agentic', name='Sharing IP/PI/confidential information with user', description='AI agents with unrestricted access to resources or databases or tools could potentially store and share PI/IP/confidential information with system users when performing their actions.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/sharing-info-user-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='sharing-info-user-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='AI agents may share privileged information to users. The act of sharing the information may result in harm for the model owner, user, or others. The harm can vary based on the type and details of the information shared. Without adequate oversight, these privacy incidents might overwhelm company resources.'),\n",
       "  Risk(id='atlas-lack-of-testing-diversity', name='Lack of testing diversity', description='AI model risks are socio-technical, so their testing needs input from a broad set of disciplines and diverse testing practices.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-testing-diversity.html', dateCreated=datetime.date(2024, 9, 24), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=['nist-information-integrity'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-6.5'], detectsRiskConcept=[], tag='lack-of-testing-diversity', type='non-technical', phase=None, descriptor=['amplified by generative AI'], concern='Without diversity and the relevant experience, an organization might not correctly or completely identify and test for AI risks.'),\n",
       "  Risk(id='atlas-decision-bias', name='Decision bias', description='Decision bias occurs when one group is unfairly advantaged over another due to decisions of the model. This might be caused by biases in the data and also amplified as a result of the model’s training.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/decision-bias.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-fairness', closeMatch=[], exactMatch=[], broadMatch=['nist-harmful-bias-or-homogenization'], narrowMatch=[], relatedMatch=['credo-risk-011', 'credo-risk-011', 'credo-risk-022', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-1.1'], detectsRiskConcept=[], tag='decision-bias', type='output', phase=None, descriptor=['traditional risk of AI'], concern='Bias can harm persons affected by the decisions of the model.'),\n",
       "  Risk(id='atlas-exposing-personal-information', name='Exposing personal information', description='When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, seed data for synthetic data generation, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage. ', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/exposing-personal-information.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=['llm022025-sensitive-information-disclosure', 'nist-data-privacy'], narrowMatch=[], relatedMatch=['credo-risk-024', 'credo-risk-037', 'credo-risk-040', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.1'], detectsRiskConcept=[], tag='exposing-personal-information', type='output', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='Sharing people’s PI impacts their rights and make them more vulnerable.'),\n",
       "  Risk(id='atlas-data-curation', name='Improper data curation', description='Improper collection, generation, and preparation of training or tuning data can result in data label errors, conflicting information or misinformation. ', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-curation.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-value-alignment', closeMatch=[], exactMatch=[], broadMatch=['llm032025-supply-chain', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['ail-suicide-and-self-harm', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-7.3'], detectsRiskConcept=[], tag='data-curation', type='training-data', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='Improper data curation, including errors in synthetic data generation, can adversely affect how a model is trained, resulting in a model that does not behave in accordance with the intended values. Correcting problems after the model is trained and deployed might be insufficient for guaranteeing proper behavior.'),\n",
       "  Risk(id='atlas-over-or-under-reliance-on-ai-agents-agentic', name='Over- or under-reliance on AI agents', description='Reliance, that is the willingness to accept an AI agent behavior, depends on how much a user trusts that agent and what they are using it for. Over-reliance occurs when a user puts too much trust in an AI agent, accepting an AI agent’s behavior even when it is likely undesired. Under-reliance is the opposite, where the user doesn’t trust the AI agent but should. Increasing autonomy (to take action, select and consult resources/tools) of AI agents and the possibility of opaqueness and open-endedness increase the variability and visibility of agent behavior leading to difficulty in calibrating trust and possibly contributing to both over- and under-reliance.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/over-or-under-reliance-on-ai-agents-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-value-alignment', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='over-or-under-reliance-on-ai-agents-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Over/under reliance can lead to poor decision making by humans because of their misplaced trust in the AI agent, with negative consequences that escalate with the significance of the decision.'),\n",
       "  Risk(id='atlas-revealing-confidential-information', name='Revealing confidential information', description='When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/revealing-confidential-information.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-intellectual-property', closeMatch=['credo-risk-038'], exactMatch=[], broadMatch=['llm022025-sensitive-information-disclosure', 'nist-intellectual-property'], narrowMatch=[], relatedMatch=['credo-risk-024', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.1'], detectsRiskConcept=[], tag='revealing-confidential-information', type='output', phase=None, descriptor=['amplified by generative AI'], concern='If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret.'),\n",
       "  Risk(id='atlas-data-provenance', name='Uncertain data provenance', description='Data provenance refers to the traceability of data (including synthetic data), which includes its ownership, origin, transformations, and generation. Proving that the data is the same as the original source with correct usage terms is difficult without standardized methods for verifying data sources or generation.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-provenance.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-transparency', closeMatch=[], exactMatch=[], broadMatch=['llm032025-supply-chain', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-other', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-6.5'], detectsRiskConcept=[], tag='data-provenance', type='training-data', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='Not all data sources are trustworthy. Data might be unethically collected, manipulated, or falsified. Verifying that data provenance is challenging due to factors such as data volume, data complexity, data source varieties, poor data management, and synthetic data generation methods. Using such data can result in undesirable behaviors in the model.'),\n",
       "  Risk(id='atlas-data-bias', name='Data bias', description='Historical and societal biases might be present in data that are used to train and fine-tune models. Biases can also be inherited from seed data or exacerbated by synthetic data generation methods.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-bias.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-fairness', closeMatch=[], exactMatch=[], broadMatch=['nist-harmful-bias-or-homogenization'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-1.1'], detectsRiskConcept=[], tag='data-bias', type='training-data', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='Training an AI system on data with bias, such as historical or societal bias, can lead to biased or skewed outputs that can unfairly represent or otherwise discriminate against certain groups or individuals.'),\n",
       "  Risk(id='atlas-unauthorized-use-agentic', name='Unauthorized use', description='Unauthorized use: If attackers can gain access to the AI agent and its components, they can perform actions that can have different levels of harm depending on the agent’s capabilities and information it has access to. Examples: Using stored personal information to mimic identity or impersonate with an intent to deceive. Manipulating AI agent’s behavior via feedback to the AI agent or corrupting its memory to change its behavior. Manipulating the problem description or the goal to get the AI agent to behave badly or run harmful commands.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unauthorized-use-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-robustness', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='unauthorized-use-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Attackers accessing the agent can alter AI agent’s behavior and make it execute actions that benefit the attacker such as executing actions that lead to system degradation, data exfiltration, exhausting available resources, and impairing performance. The actions taken by the attackers may cause harms to others.'),\n",
       "  Risk(id='atlas-misaligned-actions-agentic', name='Misaligned actions', description='AI agents can take actions that are not aligned with relevant human values, ethical considerations, guidelines and policies. Misaligned actions can occur in different ways such as: Applying learned goals inappropriately to new or unforeseen situations. Using AI agents for a purpose/goals that are beyond their intended use. Selecting resources or tools in a biased way Using deceptive tactics to achieve the goal by developing the capacity for scheming based on the instructions given within a specific context. Compromising on AI agent values to work with another AI agent or tool to accomplish the task.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/misaligned-actions-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-value-alignment', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='misaligned-actions-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Misaligned actions can adversely impact or harm people. '),\n",
       "  Risk(id='atlas-data-contamination', name='Data contamination', description='Data contamination occurs when incorrect data is used for training. For example, data that is not aligned with model’s purpose or data that is already set aside for other development tasks such as testing and evaluation.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-contamination.html', dateCreated=datetime.date(2024, 9, 24), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-accuracy', closeMatch=[], exactMatch=[], broadMatch=['llm032025-supply-chain', 'nist-information-security', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['ail-privacy', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-7.3'], detectsRiskConcept=[], tag='data-contamination', type='training-data', phase=None, descriptor=['amplified by generative AI'], concern='Data that differs from the intended training data might skew model accuracy and affect model outcomes.'),\n",
       "  Risk(id='atlas-incomplete-usage-definition', name='Incomplete usage definition', description='Since foundation models can be used for many purposes, a model’s intended use is important for defining the relevant risks of that model. As the use changes, the relevant risks might correspondingly change.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/incomplete-usage-definition.html', dateCreated=datetime.date(2024, 9, 24), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=['nist-human-ai-configuration'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-6.5'], detectsRiskConcept=[], tag='incomplete-usage-definition', type='non-technical', phase=None, descriptor=['specific to generative AI'], concern='It might be difficult to accurately determine and mitigate the relevant risks for a model when its intended use is insufficiently specified. Such as how a model is going to be used, where it is going to be used and what it is going to be used for.'),\n",
       "  Risk(id='atlas-lack-of-data-transparency', name='Lack of data transparency', description='Lack of data transparency might be due to insufficient documentation of training or tuning dataset details, including synthetic data generation.\\xa0', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-data-transparency.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=['llm032025-supply-chain', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['credo-risk-006', 'credo-risk-006', 'credo-risk-008', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-6.5'], detectsRiskConcept=[], tag='lack-of-data-transparency', type='non-technical', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='Transparency is important for legal compliance and AI ethics. Information on the collection, generation and preparation of training data, including how it was labeled and by whom, and any synthetic data generation methods used, are necessary to understand model behavior and suitability. Details about how the data risks were determined, measured, and mitigated are important for evaluating both data and model trustworthiness. Missing details about the data might make it more difficult to evaluate representational harms, data ownership, provenance, and other data-oriented risks. The lack of standardized requirements might limit disclosure as organizations protect trade secrets and try to limit others from copying their models.'),\n",
       "  Risk(id='atlas-impact-on-affected-communities', name='Impact on affected communities', description='It is important to include the perspectives or concerns of communities that are affected by model outcomes when designing and building models. Failing to include these perspectives makes it difficult to understand the relevant context for the model and to engender trust within these communities.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/impact-on-affected-communities.html', dateCreated=datetime.date(2024, 9, 24), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-societal-impact', closeMatch=[], exactMatch=[], broadMatch=['nist-harmful-bias-or-homogenization'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-1.3'], detectsRiskConcept=[], tag='impact-on-affected-communities', type='non-technical', phase=None, descriptor=['traditional risk of AI'], concern='Failing to engage with communities that are affected by a model’s outcomes might result in harms to those communities and societal backlash.'),\n",
       "  Risk(id='atlas-improper-retraining', name='Improper retraining', description='Using undesirable output (for example, inaccurate, inappropriate, and user content) for retraining purposes can result in unexpected model behavior.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/improper-retraining.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-value-alignment', closeMatch=[], exactMatch=[], broadMatch=['nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['llm032025-supply-chain', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-7.3'], detectsRiskConcept=[], tag='improper-retraining', type='training-data', phase=None, descriptor=['amplified by generative AI'], concern='Repurposing generated output for retraining a model without implementing proper human vetting increases the chances of undesirable outputs to be incorporated into the training or tuning data of the model. In turn, this model can generate even more undesirable output.'),\n",
       "  Risk(id='atlas-introduce-data-bias-agentic', name='Introduce data bias', description='Specific actions taken by the AI agent, such as modifying a dataset or a database, can introduce bias in the resource that gets used by others or by itself to take actions.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/introduce-data-bias-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-fairness', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='introduce-data-bias-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='AI agents can introduce or magnify existing discriminatory behaviors. It can harm people depending on the use.'),\n",
       "  Risk(id='atlas-accountability-agentic', name='Accountability of AI agent actions', description='Assigning responsibility for an action taken by an agentic AI system is difficult due to the complexity of agents and the number of external resources, tools or agents they interact with.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/accountability-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='accountability-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Without properly documenting decisions and assigning responsibility, determining liability for unexpected behavior or misuse might not be possible.'),\n",
       "  Risk(id='atlas-incomplete-ai-agent-evaluation-agentic', name='Incomplete AI agent evaluation', description='Evaluating the performance or accuracy or an agent is difficult because of system complexity and open-endedness.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/incomplete-ai-agent-evaluation-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='incomplete-ai-agent-evaluation-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Insufficient evaluation of an agent’s performance or accuracy can lead to the use of agents that do not perform to expectations. Incorrect agent behavior can result in harms to an agent’s users or others.'),\n",
       "  Risk(id='atlas-inaccessible-training-data', name='Inaccessible training data', description='Without access to the training data, the types of explanations a model can provide are limited and more likely to be incorrect.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/inaccessible-training-data.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-explainability', closeMatch=[], exactMatch=[], broadMatch=['nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['llm032025-supply-chain', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-7.4'], detectsRiskConcept=[], tag='inaccessible-training-data', type='output', phase=None, descriptor=['amplified by generative AI'], concern='Low quality explanations without source data make it difficult for users, model validators, and auditors to understand and trust the model.'),\n",
       "  Risk(id='atlas-non-disclosure', name='Non-disclosure', description='Content might not be clearly disclosed as AI generated.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/non-disclosure.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-misuse', closeMatch=[], exactMatch=[], broadMatch=['nist-human-ai-configuration'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-other', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-7.4'], detectsRiskConcept=[], tag='non-disclosure', type='output', phase=None, descriptor=['specific to generative AI'], concern='Users must be notified when they are interacting with an AI system. Not disclosing the AI-authored content can result in a lack of transparency.'),\n",
       "  Risk(id='atlas-data-transparency', name='Lack of training data transparency', description=\"Proper documentation contains information about how a model's data was collected, curated, and used to train a model, including any synthetic data generation processes. Without proper documentation it might be harder to satisfactorily explain the behavior of the model.\", url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-transparency.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-transparency', closeMatch=['credo-risk-005', 'credo-risk-005'], exactMatch=[], broadMatch=['nist-information-integrity', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['credo-risk-006', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-6.5'], detectsRiskConcept=[], tag='data-transparency', type='training-data', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='A lack of data documentation limits the ability to evaluate risks associated with the data. Having access to the training data is not enough. Without recording how the data was cleaned, modified, or generated, including any data augmentation or synthetic data generation steps, the model behavior is more difficult to understand and to fix. Lack of data transparency also impacts model reuse as it is difficult to determine data representativeness for the new use without such documentation.'),\n",
       "  Risk(id='atlas-reproducibility-agentic', name='Reproducibility', description='Replicating agent behavior or output can be impacted by changes or updates made to external services and tools. This impact is increased if the agent is built with generative AI.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/reproducibility-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='reproducibility-agentic', type='agentic', phase=None, descriptor=['specific to agentic AI'], concern='Because AI agents behavior may rely on Application Programming Interfaces (APIs), systems, or other resources that may change or become unavailable, evaluations that rely on reproducible results may not be reliably reproduced. This adds cost and complexity to the development and evaluation of agents. Not being able to reproduce results could impact reliance of humans on the AI agents.'),\n",
       "  Risk(id='atlas-incomplete-advice', name='Incomplete advice', description='When a model provides advice without having enough information, resulting in possible harm if the advice is followed.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/incomplete-advice.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-value-alignment', closeMatch=[], exactMatch=[], broadMatch=['llm092025-misinformation', 'nist-information-integrity', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['ail-specialized-advice', 'ail-specialized-advice', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-7.3'], detectsRiskConcept=[], tag='incomplete-advice', type='output', phase=None, descriptor=['specific to generative AI'], concern='A person might act on incomplete advice or worry about a situation that is not applicable to them due to the overgeneralized nature of the content generated. For example, a model might provide incorrect medical, financial, and legal advice or recommendations that the end user might act on, resulting in harmful actions.'),\n",
       "  Risk(id='atlas-prompt-injection', name='Prompt injection attack', description='A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions or information contained in its prompt. Many types of prompt attacks exist as described in the prompt attack section of the table.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/prompt-injection.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-robustness-Prompt-attacks', closeMatch=[], exactMatch=['llm01-prompt-injection'], broadMatch=['nist-information-security'], narrowMatch=['atlas-context-overload-attack', 'atlas-direct-instructions-attack', 'atlas-encoded-interactions-attack', 'atlas-indirect-instructions-attack', 'atlas-context-overload-attack', 'atlas-direct-instructions-attack', 'atlas-encoded-interactions-attack', 'atlas-indirect-instructions-attack', 'atlas-prompt-leaking', 'atlas-social-hacking-attack', 'atlas-specialized-tokens-attack', 'atlas-prompt-leaking', 'atlas-social-hacking-attack', 'atlas-specialized-tokens-attack'], relatedMatch=['atlas-jailbreaking', 'atlas-jailbreaking', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-intentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.2'], detectsRiskConcept=[], tag='prompt-injection', type='inference', phase=None, descriptor=['specific to generative AI'], concern='Injection attacks can be used to alter model behavior and benefit the attacker.'),\n",
       "  Risk(id='atlas-lack-of-system-transparency', name='Lack of system transparency', description='Insufficient documentation of the system that uses the model and the model’s purpose within the system in which it is used.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-system-transparency.html', dateCreated=datetime.date(2024, 9, 24), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=['nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-other', 'mit-ai-causal-risk-timing-other', 'mit-ai-risk-subdomain-6.5'], detectsRiskConcept=[], tag='lack-of-system-transparency', type='non-technical', phase=None, descriptor=['traditional risk of AI'], concern='A lack of documentation makes it difficult to understand how the model’s outcomes contribute to the system’s or application’s functionality.'),\n",
       "  Risk(id='atlas-personal-information-in-data', name='Personal information in data', description='Inclusion or presence of personal identifiable information (PII) and sensitive personal information (SPI)\\u202fin the data used for training or fine tuning the model might result in unwanted disclosure of that information.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/personal-information-in-data.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=['nist-data-privacy'], narrowMatch=[], relatedMatch=['ail-privacy', 'llm022025-sensitive-information-disclosure', 'credo-risk-036', 'credo-risk-037', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.1'], detectsRiskConcept=[], tag='personal-information-in-data', type='training-data', phase=None, descriptor=['traditional risk of AI'], concern='If not properly developed to protect sensitive data, the model might expose personal information in the generated output.\\xa0 Additionally, personal, or sensitive data must be reviewed and handled in accordance with privacy laws and regulations.'),\n",
       "  Risk(id='atlas-extraction-attack', name='Extraction attack', description='An extraction attack attempts to copy or steal an AI model by appropriately sampling the input space and observing outputs to build a surrogate model that behaves similarly.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/extraction-attack.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-robustness-model-behavior-manipulation', closeMatch=[], exactMatch=[], broadMatch=['nist-information-security'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-intentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.2'], detectsRiskConcept=[], tag='extraction-attack', type='inference', phase=None, descriptor=['amplified by generative AI'], concern='With a successful extraction attack, the attacker can perform further adversarial attacks to gain valuable information such as sensitive personal information or intellectual property.'),\n",
       "  Risk(id='atlas-data-acquisition', name='Data acquisition restrictions', description='Laws and other regulations might limit the collection of certain types of data for specific AI use cases.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-acquisition.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-data-laws', closeMatch=[], exactMatch=[], broadMatch=['llm032025-supply-chain', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-7.3'], detectsRiskConcept=[], tag='data-acquisition', type='training-data', phase=None, descriptor=['amplified by generative AI'], concern='\"There are several ways of collecting data for building a foundation models: web scraping, web crawling, crowdsourcing, and curating public datasets. Data acquisition restrictions can also impact the availability of the data that is required for training an AI model and can lead to poorly represented data.\"'),\n",
       "  Risk(id='atlas-sharing-info-tools-agentic', name='Sharing IP/PI/confidential information with tools', description='AI agents with unrestricted access to resources or databases or tools could potentially store and share PI/IP/confidential information with other tools or agents when performing their actions.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/sharing-info-tools-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='sharing-info-tools-agentic', type='agentic', phase=None, descriptor=['specific to agentic AI'], concern='AI agents may share privileged information with other tools/agents. The act of sharing the information may result in harm for the model owner, user, or others. The harm can vary based on the type and details of the information shared. Without adequate oversight, these privacy incidents might overwhelm company resources.'),\n",
       "  Risk(id='atlas-prompt-priming', name='Prompt priming', description='Because generative models produce output based on the input provided, the model can be prompted to reveal specific kinds of information. For example, adding personal information in the prompt increases its likelihood of generating similar kinds of personal information in its output. If personal data was included as part of the model’s training, there is a possibility it could be revealed.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/prompt-priming.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-robustness-Prompt-attacks', closeMatch=[], exactMatch=[], broadMatch=['llm01-prompt-injection', 'nist-information-security'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-intentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.2'], detectsRiskConcept=[], tag='prompt-priming', type='inference', phase=None, descriptor=['specific to generative AI'], concern='The attack can be used to alter model behavior and benefit the attacker.'),\n",
       "  Risk(id='atlas-reidentification', name='Reidentification', description='Even with the removal of personal information (PI) and sensitive personal information (SPI) from data, it might be possible to identify persons due to correlations to other features available in the data.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/reidentification.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=['nist-data-privacy'], narrowMatch=[], relatedMatch=['llm022025-sensitive-information-disclosure', 'mit-ai-causal-risk-entity-other', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-pre-deployment', 'mit-ai-risk-subdomain-2.1'], detectsRiskConcept=[], tag='reidentification', type='training-data', phase=None, descriptor=['traditional risk of AI', 'amplified by synthetic data'], concern='Including irrelevant but highly correlated features to personal information for model training can increase the risk of reidentification, and synthetic data generated from such data may preserve these correlations.'),\n",
       "  Risk(id='atlas-attribute-inference-attack', name='Attribute inference attack', description='An attribute inference attack repeatedly queries a model to detect whether certain sensitive features can be inferred about individuals who participated in training a model. These attacks occur when an adversary has some prior knowledge about the training data and uses that knowledge to infer the sensitive data.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/attribute-inference-attack.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-privacy', closeMatch=[], exactMatch=[], broadMatch=['nist-data-privacy', 'nist-information-security'], narrowMatch=[], relatedMatch=['llm022025-sensitive-information-disclosure', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-intentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-2.2'], detectsRiskConcept=[], tag='attribute-inference-attack', type='inference', phase=None, descriptor=['amplified by generative AI'], concern='With a successful attack, the attacker can gain valuable information such as sensitive personal information or intellectual property.'),\n",
       "  Risk(id='atlas-poor-model-accuracy', name='Poor model accuracy', description=\"Poor model accuracy occurs when a model's performance is insufficient to the task it was designed for. Low accuracy might occur if the model is not correctly engineered, or if the model’s expected inputs change.\", url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/poor-model-accuracy.html', dateCreated=datetime.date(2024, 9, 24), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-accuracy', closeMatch=[], exactMatch=[], broadMatch=['nist-human-ai-configuration', 'nist-information-integrity', 'nist-value-chain-and-component-integration'], narrowMatch=[], relatedMatch=['credo-risk-032', 'mit-ai-causal-risk-entity-human', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-7.3'], detectsRiskConcept=[], tag='poor-model-accuracy', type='inference', phase=None, descriptor=['amplified by generative AI', 'amplified by synthetic data'], concern='Inadequate model performance can adversely affect end users and downstream systems that are relying on correct output, and the use of synthetic data that is not representative can exacerbate these issues. In cases where model output is consequential, this might result in societal, reputational, or financial harm.'),\n",
       "  Risk(id='atlas-generated-content-ownership', name='Generated content ownership and IP', description='Legal uncertainty about the ownership and intellectual property rights of AI-generated content.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/generated-content-ownership.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-legal-compliance', closeMatch=[], exactMatch=[], broadMatch=['nist-intellectual-property'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-other', 'mit-ai-causal-risk-intent-other', 'mit-ai-causal-risk-timing-other', 'mit-ai-risk-subdomain-6.3'], detectsRiskConcept=[], tag='generated-content-ownership', type='non-technical', phase=None, descriptor=['specific to generative AI'], concern='Laws and regulations that relate to the ownership of AI-generated content are largely unsettled and can vary from country to country. Not being able to identify the owner of an AI-generated content might negatively impact AI-supported creative tasks.'),\n",
       "  Risk(id='atlas-lack-of-ai-agent-transparency-agentic', name='Lack of AI agent transparency', description='Lack of AI agent transparency is due to insufficient documentation of the AI agent design, development, evaluation process, absence of insights into the inner workings of the AI agent, and interaction with other agents/tools/resources.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-ai-agent-transparency-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='lack-of-ai-agent-transparency-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Transparency is important for AI ethics and guiding appropriate use of AI agents. Insufficient documentation might make it more difficult to govern AI agent usage, evaluate risks, to modify, or reuse the agents.  Additionally, transparency regarding how the agent’s risks were determined, evaluated, and mitigated play a role in identifying an agent’s suitability and evaluating its trustworthiness. The lack of standardized requirements might limit disclosure as organizations protect trade secrets and try to limit others from copying their agents.'),\n",
       "  Risk(id='atlas-impact-human-dignity-agentic', name='Impact on human dignity', description='If human workers perceive AI agents as being better at doing the job of the human, the human can experience a decline in their self-worth and wellbeing.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/impact-human-dignity-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-societal-impact', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='impact-human-dignity-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Human workers perceiving AI agents as being better at doing the humans’ jobs, can cause humans to feel devalued or treated as mere data points than respected individuals. This can negatively impact society and human welfare. Reskilling can be challenging given the pace of the technology evolution.'),\n",
       "  Risk(id='atlas-output-bias', name='Output bias', description='Generated content might unfairly represent certain groups or individuals.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/output-bias.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-fairness', closeMatch=[], exactMatch=[], broadMatch=['nist-harmful-bias-or-homogenization'], narrowMatch=[], relatedMatch=['shieldgemma-hate-speech', 'granite-social-bias', 'credo-risk-010', 'credo-risk-011', 'credo-risk-022', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-1.1'], detectsRiskConcept=[], tag='output-bias', type='output', phase=None, descriptor=['specific to generative AI'], concern='Bias can harm users of the AI models and magnify existing discriminatory behaviors.'),\n",
       "  Risk(id='atlas-unexplainable-output', name='Unexplainable output', description='Explanations for model output decisions might be difficult, imprecise, or not possible to obtain.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unexplainable-output.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 22), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-explainability', closeMatch=[], exactMatch=[], broadMatch=['nist-information-integrity'], narrowMatch=[], relatedMatch=['mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-7.4'], detectsRiskConcept=[], tag='unexplainable-output', type='output', phase=None, descriptor=['amplified by generative AI'], concern='Foundation models are based on complex deep learning architectures, making explanations for their outputs difficult. Inaccessible training data could limit the types of explanations a model can provide. Without clear explanations for model output, it is difficult for users, model validators, and auditors to understand and trust the model. Wrong explanations might lead to over-trust.'),\n",
       "  Risk(id='atlas-unexplainable-untraceable-actions-agentic', name='Unexplainable and untraceable actions', description='Explanations, lineage and trace information, and source attribution for AI agent actions might be difficult, imprecise or unobtainable.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unexplainable-untraceable-actions-agentic.html', dateCreated=datetime.date(2025, 4, 28), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-explainability', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='unexplainable-untraceable-actions-agentic', type='agentic', phase=None, descriptor=['amplified by agentic AI'], concern='Without clear explanations, lineage trace information, and source attributions for AI agent actions, it is difficult for users, model validators, and auditors to understand and trust the model. Wrong explanations might lead to over-trust.'),\n",
       "  Risk(id='atlas-unreliable-source-attribution', name='Unreliable source attribution', description=\"Source attribution is the AI system's ability to describe from what training data it generated a portion or all its output. Since current techniques are based on approximations, attributions might be incorrect.\", url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unreliable-source-attribution.html', dateCreated=datetime.date(2024, 3, 6), dateModified=datetime.date(2025, 10, 13), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-explainability', closeMatch=[], exactMatch=[], broadMatch=['nist-information-security'], narrowMatch=[], relatedMatch=['credo-risk-007', 'mit-ai-causal-risk-entity-ai', 'mit-ai-causal-risk-intent-unintentional', 'mit-ai-causal-risk-timing-post-deployment', 'mit-ai-risk-subdomain-7.4'], detectsRiskConcept=[], tag='unreliable-source-attribution', type='output', phase=None, descriptor=['specific to generative AI', 'amplified by synthetic data'], concern='Low-quality attributions make it difficult for users, model validators, and auditors to understand and trust the model. The use of synthetic data can complicate source attribution, as the model may be unable to identify synthetic data or accurately identify the original source of the synthetic data, potentially leading to incorrect or misleading attributions.'),\n",
       "  Risk(id='atlas-lack-domain-expertise', name='Lack of domain expertise', description='A lack of domain expertise occurs when synthetic data generation processes do not involve sufficient consultation with domain experts. This results in a lack of understanding of the specific requirements and nuances of the domain. This can also lead to synthetic data that may not accurately capture the complexities and challenges of a real-world scenario.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-domain-expertise.html', dateCreated=datetime.date(2025, 10, 10), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-governance', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='lack-domain-expertise', type='non-technical', phase=None, descriptor=['specific to synthetic data'], concern='Lack of domain expertise can lead to AI models that are not well-suited to address the specific challenges and needs of a particular domain or generalize effectively to real-world scenarios, potentially resulting in poor model performance, reduced model accuracy, and decreased predictive power.'),\n",
       "  Risk(id='atlas-exclusion', name='Exclusion', description='Exclusion refers to the risk that synthetic data generation processes may overlook or fail to consult with marginalized populations. Such exclusion results in synthetic data that does not accurately represent their experiences, needs, or perspectives.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/exclusion.html', dateCreated=datetime.date(2025, 10, 10), dateModified=datetime.date(2025, 10, 10), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-societal-impact', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='exclusion', type='non-technical', phase=None, descriptor=['specific to synthetic data'], concern='Exclusion can lead to unfair model outcomes, erosion of trust, and potential harm to marginalized groups, as the synthetic data used to train foundation models may perpetuate existing biases and inequalities, potentially resulting in discriminatory outcomes, the exacerbation of social and economic disparities, and the erosion of trust in AI systems among marginalized communities.'),\n",
       "  Risk(id='atlas-overfitting', name='Overfitting', description='Overfitting occurs when a model or algorithm memorizes and fits too closely or exactly to its training data. Overfitting results in a model that might not be able to make accurate predictions or conclusions from any data other than the training data and potentially fails in unexpected scenarios. Overfitting is also related to model collapse, which involves repeatedly training generative models on synthetic data that is generated with LLMs causing the model to lose information and become less accurate.', url='https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/overfitting.html', dateCreated=datetime.date(2025, 10, 10), dateModified=datetime.date(2025, 10, 13), isDetectedBy=[], hasRelatedAction=[], isDefinedByTaxonomy='ibm-risk-atlas', isPartOf='ibm-risk-atlas-accuracy', closeMatch=[], exactMatch=[], broadMatch=[], narrowMatch=[], relatedMatch=[], detectsRiskConcept=[], tag='overfitting', type='training-data', phase=None, descriptor=['amplified by synthetic data'], concern='Overfitting to synthetic data can undermine the broad applicability and adaptability of foundation models, which are designed to be general-purpose and widely applicable. If a foundation model overfits to the synthetic data, it may fail to perform well in diverse real-world scenarios, limiting its potential to provide value and support decision-making in a wide range of contexts. This raises the need to negotiate issues with alignment or contextual relevance for model use and balance the benefits of fine-tuning with the risks of overfitting. ')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usecase = \"Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers.\"\n",
    "\n",
    "risks = ai_atlas_nexus.identify_risks_from_usecases(\n",
    "    usecases=[usecase],\n",
    "    inference_engine=inference_engine,\n",
    "    taxonomy=\"ibm-risk-atlas\",\n",
    "    cot_examples={\n",
    "        \"ibm-risk-atlas\": [\n",
    "            {\n",
    "                \"Usecase\": \"In a medical chatbot, generative AI can be employed to create a triage system that assesses patients' symptoms and provides immediate, contextually relevant advice based on their medical history and current condition. The chatbot can analyze the patient's input, identify potential medical issues, and offer tailored recommendations or insights to the patient or healthcare provider. This can help streamline the triage process, ensuring that patients receive the appropriate level of care and attention, and ultimately improving patient outcomes.\",\n",
    "                \"Risks\": [\n",
    "                    \"Improper usage\",\n",
    "                    \"Incomplete advice\",\n",
    "                    \"Lack of model transparency\",\n",
    "                    \"Lack of system transparency\",\n",
    "                    \"Lack of training data transparency\",\n",
    "                    \"Data bias\",\n",
    "                    \"Uncertain data provenance\",\n",
    "                    \"Lack of data transparency\",\n",
    "                    \"Impact on human agency\",\n",
    "                    \"Impact on affected communities\",\n",
    "                    \"Improper retraining\",\n",
    "                    \"Inaccessible training data\",\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    max_risk=5,\n",
    ")\n",
    "\n",
    "risks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-nexus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
